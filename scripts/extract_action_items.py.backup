#!/usr/bin/env python3
"""Extract action items from session summaries using structured LLM extraction.

This script implements Phase 2 of the Session Summary Mining initiative:
- Pydantic schemas for structured action item extraction
- Section-based extraction for granular context preservation
- Multi-level validation and error handling
- SQLite logging for quality tracking

Usage:
    # Extract from single file
    python scripts/extract_action_items.py docs/archive/session-summaries/2025-10-20-*.md

    # Extract from date range
    python scripts/extract_action_items.py --date-range 2025-10-15:2025-10-19

    # Extract from all summaries
    python scripts/extract_action_items.py --all

    # Save to YAML file (30% more token-efficient than JSON)
    python scripts/extract_action_items.py --all --output action-items.yaml
"""

import argparse
import asyncio
import hashlib
import re
import sqlite3
import sys
from datetime import date, datetime
from pathlib import Path
from typing import Literal, Optional

import instructor
import yaml
from openai import OpenAI
from pydantic import BaseModel, Field


# ============================================================================
# Pydantic Schemas (Section 2.1)
# ============================================================================


class ActionItem(BaseModel):
    """Structured action item extracted from session summaries.

    Follows Instructor pattern for reliable LLM extraction with validation.
    """

    id: str = Field(
        description="Unique ID: {date}-{summary}#{section}#{index} (e.g., '2025-10-18-summary#improvements#1')"
    )
    title: str = Field(
        description="Concise title (5-10 words) describing the action"
    )
    description: str = Field(
        description="Detailed description with context (1-3 sentences)"
    )
    category: Literal[
        "missing_capability",
        "pain_point",
        "regression",
        "improvement",
        "technical_debt",
        "documentation",
        "testing",
        "automation",
        "security",
        "performance",
    ] = Field(description="Primary category of this action item")
    impact: Literal["high", "medium", "low"] = Field(
        description="Estimated impact on project success"
    )
    confidence: Literal["high", "medium", "low"] = Field(
        description="Confidence in extraction accuracy and actionability"
    )
    source_summary: str = Field(
        description="Source session summary filename"
    )
    source_section: str = Field(
        description="Section header where item was found"
    )
    session_date: date = Field(
        description="Date of session summary (from filename)"
    )
    related_files: list[str] = Field(
        default_factory=list,
        description="Files mentioned in relation to this action item",
    )
    blockers: Optional[list[str]] = Field(
        default=None,
        description="Dependencies or blockers preventing completion",
    )
    initiative_match: Optional[str] = Field(
        default=None,
        description="Matching active initiative (if any)",
    )


# ============================================================================
# Extraction Prompts (Section 2.2)
# ============================================================================


SYSTEM_PROMPT = """You are an expert at extracting actionable insights from technical session summaries.

Your task is to identify:
1. **Missing Capabilities** - Features/tools that don't exist yet
2. **Pain Points** - Friction, inefficiencies, manual work
3. **Regressions** - Things that broke or got worse
4. **Improvements** - Optimization opportunities
5. **Technical Debt** - Code quality, architecture issues
6. **Documentation Gaps** - Missing or outdated docs
7. **Testing Gaps** - Untested scenarios
8. **Automation Opportunities** - Repetitive manual tasks
9. **Security Issues** - Vulnerabilities, compliance gaps
10. **Performance Issues** - Slow operations, inefficiencies

**Classification Criteria:**

**Impact:**
- High: Blocks work, affects many users, critical path
- Medium: Improves productivity, affects some workflows
- Low: Nice-to-have, minor improvement

**Confidence:**
- High: Explicit mention with clear description
- Medium: Implied or partially described
- Low: Uncertain extraction or vague reference

**Important:**
- Extract concrete, actionable items (not generic observations)
- Preserve context from source (quote key phrases)
- Include file paths mentioned
- Note blockers/dependencies if mentioned
- Skip items that are already completed (‚úÖ or "Done")
"""


def create_user_prompt(section: str, metadata: dict[str, str]) -> str:
    """Create user prompt for extraction from a specific section.


async def process_summaries(
    file_paths: list[Path],
    output_file: Path | None = None,
) -> None:
    """Process multiple session summaries and extract action items.

    Args:
        file_paths: List of session summary files to process
        output_file: Optional output file for results
    """
    # Configure MCP pipeline
    config = Config()
    config.fetcher.enable_file_system = True
    config.fetcher.allowed_directories = [
        "docs/archive/session-summaries",
        str(Path.cwd() / "docs" / "archive" / "session-summaries"),
    ]

    # Disable streaming for cleaner output
    config.summarizer.streaming = False

    pipeline = WebSummarizationPipeline(config)

    # Output header
    output_lines = [
        "# Extracted Action Items\n",
        f"\n**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        f"**Source Files:** {len(file_paths)}\n",
        "\n---\n\n",
    ]

    print("üîç Extracting action items from session summaries...\n")

    # Process each file
    for i, file_path in enumerate(file_paths, 1):
        print(f"[{i}/{len(file_paths)}] Processing: {file_path.name}")

        try:
            result = await extract_action_items_from_file(pipeline, file_path)

            # Add to output
            output_lines.append(f"## {file_path.stem}\n\n")
            output_lines.append(f"**File:** `{file_path.name}`\n\n")
            output_lines.append(result)
            output_lines.append("\n\n---\n\n")

        except Exception as e:
            error_msg = f"**Error processing {file_path.name}:** {str(e)}\n\n"
            output_lines.append(error_msg)
            print(f"   ‚ùå Error: {str(e)}")
            continue

    # Write output
    output_text = "".join(output_lines)

    if output_file:
        output_file.write_text(output_text)
        print(f"\n‚úÖ Extracted action items written to: {output_file}")
    else:
        print("\n" + "=" * 80 + "\n")
        print(output_text)


def find_summaries_by_date_range(start_date: date, end_date: date) -> list[Path]:
    """Find session summaries within date range.

    Args:
        start_date: Start date (inclusive)
        end_date: End date (inclusive)

    Returns:
        List of matching session summary files
    """
    summaries_dir = Path("docs/archive/session-summaries")
    if not summaries_dir.exists():
        print(f"‚ùå Error: Directory not found: {summaries_dir}")
        return []

    matching_files: list[Path] = []

    for file_path in summaries_dir.glob("*.md"):
        # Parse date from filename (format: YYYY-MM-DD-*.md)
        try:
            date_str = file_path.name[:10]  # Extract YYYY-MM-DD
            file_date = datetime.strptime(date_str, "%Y-%m-%d").date()

            if start_date <= file_date <= end_date:
                matching_files.append(file_path)
        except ValueError:
            # Skip files that don't match date format
            continue

    return sorted(matching_files)


def parse_date_range(date_range_str: str) -> tuple[date, date]:
    """Parse date range string.

    Args:
        date_range_str: Date range in format "YYYY-MM-DD:YYYY-MM-DD"

    Returns:
        Tuple of (start_date, end_date)

    Raises:
        ValueError: If format is invalid
    """
    try:
        start_str, end_str = date_range_str.split(":")
        start_date = datetime.strptime(start_str.strip(), "%Y-%m-%d").date()
        end_date = datetime.strptime(end_str.strip(), "%Y-%m-%d").date()

        if start_date > end_date:
            raise ValueError("Start date must be before end date")

        return start_date, end_date
    except Exception as e:
        raise ValueError("Invalid date range format. Use YYYY-MM-DD:YYYY-MM-DD") from e


def main() -> int:
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Extract action items from session summaries using MCP server",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Single file
  python scripts/extract_action_items.py docs/archive/session-summaries/2025-10-20-*.md

  # Date range
  python scripts/extract_action_items.py --date-range 2025-10-15:2025-10-19

  # All summaries
  python scripts/extract_action_items.py --all --output action-items.md
        """,
    )

    parser.add_argument(
        "files",
        nargs="*",
        help="Session summary files to process",
    )

    parser.add_argument(
        "--date-range",
        help="Extract from date range (format: YYYY-MM-DD:YYYY-MM-DD)",
    )

    parser.add_argument(
        "--all",
        action="store_true",
        help="Process all session summaries",
    )

    parser.add_argument(
        "--output",
        "-o",
        type=Path,
        help="Output file for extracted action items",
    )

    args = parser.parse_args()

    # Determine which files to process
    file_paths: list[Path] = []

    if args.all:
        summaries_dir = Path("docs/archive/session-summaries")
        if summaries_dir.exists():
            file_paths = sorted(summaries_dir.glob("*.md"))
        else:
            print(f"‚ùå Error: Directory not found: {summaries_dir}")
            return 1

    elif args.date_range:
        try:
            start_date, end_date = parse_date_range(args.date_range)
            print(f"üìÖ Searching for summaries from {start_date} to {end_date}")
            file_paths = find_summaries_by_date_range(start_date, end_date)

            if not file_paths:
                print("‚ùå No session summaries found in date range")
                return 1

            print(f"‚úÖ Found {len(file_paths)} summaries")

        except ValueError as e:
            print(f"‚ùå Error: {e}")
            return 1

    elif args.files:
        for file_arg in args.files:
            path = Path(file_arg)
            if path.exists():
                file_paths.append(path)
            else:
                print(f"‚ö†Ô∏è  Warning: File not found: {file_arg}")

    else:
        parser.print_help()
        return 1

    if not file_paths:
        print("‚ùå No valid files to process")
        return 1

    # Process summaries
    asyncio.run(process_summaries(file_paths, args.output))

    return 0


if __name__ == "__main__":
    sys.exit(main())
