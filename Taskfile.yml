version: '3'

# Taskfile for mcp-web project (uv-based workflow)
# Install task: https://taskfile.dev/installation/
# Install uv: https://docs.astral.sh/uv/
# Usage: task <command>

vars:
  UV: uv
  SRC_DIR: src
  TEST_DIR: tests
  COVERAGE_MIN: 90
  PARALLEL_WORKERS: auto

env:
  PYTHONPATH: "{{.ROOT_DIR}}/{{.SRC_DIR}}"

tasks:
  default:
    desc: Show available tasks
    cmds:
      - task --list

  # Installation tasks
  install:
    desc: Install package and all dependencies using uv
    cmds:
      - "{{.UV}} sync --all-extras"
      - task: install:playwright

  install:playwright:
    desc: Install Playwright browsers
    cmds:
      - "{{.UV}} run playwright install chromium"

  install:pre-commit:
    desc: Install pre-commit hooks
    cmds:
      - "{{.UV}} run pre-commit install"

  # Testing tasks
  test:
    desc: Run all tests (except live tests) - parallel by default
    cmds:
      - "{{.UV}} run pytest -n {{.PARALLEL_WORKERS}} -m 'not live and not benchmark' -v"

  test:fast:
    desc: Run fast tests only (exclude slow, live, benchmark)
    cmds:
      - "{{.UV}} run pytest -n {{.PARALLEL_WORKERS}} -m 'not slow and not live and not benchmark' -v"

  test:unit:
    desc: Run unit tests only
    cmds:
      - "{{.UV}} run pytest -m unit -v"

  test:unit:parallel:
    desc: Run unit tests in parallel using pytest-xdist
    cmds:
      - "{{.UV}} run pytest -n {{.PARALLEL_WORKERS}} -m unit -v"

  test:security:
    desc: Run security tests
    cmds:
      - "{{.UV}} run pytest -m security -v"

  test:golden:
    desc: Run golden/regression tests
    cmds:
      - "{{.UV}} run pytest -m golden -v"

  test:integration:
    desc: Run integration tests
    cmds:
      - "{{.UV}} run pytest -m integration -v"

  test:integration:parallel:
    desc: Run integration tests in parallel
    cmds:
      - "{{.UV}} run pytest -n {{.PARALLEL_WORKERS}} -m integration -v"

  test:live:
    desc: Run live tests (requires network and API key)
    cmds:
      - "{{.UV}} run pytest -m live -v"

  test:bench:
    desc: Run performance benchmarks (parallel with pytest-xdist)
    cmds:
      - "{{.UV}} run pytest -m benchmark --benchmark-only -v -n {{.PARALLEL_WORKERS}} --dist loadgroup"

  test:bench:single:
    desc: Run performance benchmarks (single process for accurate timing)
    cmds:
      - "{{.UV}} run pytest -m benchmark --benchmark-only -v -o addopts=''"

  test:bench:save:
    desc: Run benchmarks and save results for comparison
    cmds:
      - "{{.UV}} run pytest -m benchmark --benchmark-only --benchmark-autosave -v -o addopts=''"

  test:bench:regression:
    desc: Run benchmarks and compare against baseline (fails if >20% slower)
    cmds:
      - "{{.UV}} run pytest -m benchmark --benchmark-only --benchmark-json=output.json -v -o addopts=''"
      - "{{.UV}} run python scripts/check_performance_regression.py output.json .benchmarks/baseline.json --threshold 1.2"

  test:bench:baseline:
    desc: Generate new performance baseline
    cmds:
      - "{{.UV}} run pytest -m benchmark --benchmark-only --benchmark-json=.benchmarks/baseline.json -v -o addopts=''"
      - echo "✓ Baseline saved to .benchmarks/baseline.json"

  test:all:
    desc: Run all tests including live tests (requires API keys)
    cmds:
      - "{{.UV}} run pytest -v"

  test:all:parallel:
    desc: Run all tests in parallel
    cmds:
      - "{{.UV}} run pytest -n {{.PARALLEL_WORKERS}} -v"

  test:manual:
    desc: Manual URL summarization test (URL=url QUERY=query)
    cmds:
      - "{{.UV}} run python -m mcp_web.cli test-summarize {{.URL}} {{if .QUERY}}--query \"{{.QUERY}}\"{{end}} {{if .OUTPUT}}-o {{.OUTPUT}}{{end}} --verbose"

  test:fetch:robots:
    desc: Test URL with robots.txt handling
    cmds:
      - "{{.UV}} run python -m mcp_web.cli test-robots {{.URL}} {{if .IGNORE}}--ignore{{end}}"

  test:fast:parallel:
    desc: Run fast tests in parallel
    cmds:
      - "{{.UV}} run pytest -n {{.PARALLEL_WORKERS}} -m 'unit or security or golden' -v"

  test:parallel:
    desc: Run tests in parallel (default all except live)
    cmds:
      - "{{.UV}} run pytest -n {{.PARALLEL_WORKERS}} -m 'not live' -v"

  test:coverage:
    desc: Run tests with coverage report
    cmds:
      - "{{.UV}} run pytest --cov={{.SRC_DIR}}/mcp_web --cov-report=term-missing --cov-report=html"
      - echo "Coverage report generated in htmlcov/index.html"

  test:coverage:parallel:
    desc: Run tests with coverage in parallel
    cmds:
      - "{{.UV}} run pytest -n {{.PARALLEL_WORKERS}} --cov={{.SRC_DIR}}/mcp_web --cov-report=term-missing --cov-report=html"

  test:coverage:min:
    desc: Run tests with minimum coverage enforcement
    cmds:
      - "{{.UV}} run pytest --cov={{.SRC_DIR}}/mcp_web --cov-report=term-missing --cov-fail-under={{.COVERAGE_MIN}}"

  test:watch:
    desc: Run tests in watch mode (requires pytest-watch)
    cmds:
      - "{{.UV}} run ptw -- -m 'not live' -v"

  # Code quality tasks
  lint:
    desc: Run all linting checks
    deps:
      - lint:ruff
      - lint:format
      - lint:mypy

  lint:ruff:
    desc: Run ruff linter
    cmds:
      - "{{.UV}} run ruff check {{.SRC_DIR}} {{.TEST_DIR}}"

  lint:format:
    desc: Check code formatting
    cmds:
      - '{{.UV}} run ruff format --check {{.SRC_DIR}} {{.TEST_DIR}}'

  lint:mypy:
    desc: Run type checking
    cmds:
      - "{{.UV}} run mypy {{.SRC_DIR}} --ignore-missing-imports"

  lint:all:
    desc: Run all linting and quality checks
    deps:
      - lint
      - docs:lint

  # Code formatting tasks
  format:
    desc: Auto-format code with ruff
    cmds:
      - "{{.UV}} run ruff format {{.SRC_DIR}} {{.TEST_DIR}}"
      - "{{.UV}} run ruff check --fix {{.SRC_DIR}} {{.TEST_DIR}}"

  # Security tasks
  security:
    desc: Run all security checks
    deps:
      - security:bandit
      - security:semgrep
      - security:safety

  security:bandit:
    desc: Run Bandit security scanner
    cmds:
      - "{{.UV}} run bandit -r {{.SRC_DIR}} -c .bandit -f screen"

  security:semgrep:
    desc: Run Semgrep pattern scanner
    cmds:
      - "{{.UV}} run semgrep --config=.semgrep.yml {{.SRC_DIR}} --error"

  security:safety:
    desc: Check for known vulnerabilities in dependencies
    cmds:
      - "{{.UV}} run safety check --json"

  # Quality analysis
  analyze:
    desc: Run complete static analysis
    deps:
      - lint
      - security

  analyze:complexity:
    desc: Analyze code complexity (requires radon)
    cmds:
      - "{{.UV}} run radon cc {{.SRC_DIR}} -a -nb"

  analyze:maintainability:
    desc: Analyze maintainability index (requires radon)
    cmds:
      - "{{.UV}} run radon mi {{.SRC_DIR}} -nb"

  # Documentation tasks
  docs:build:
    desc: Build documentation (if using Sphinx/MkDocs)
    cmds:
      - echo "Documentation build not yet configured"

  docs:serve:
    desc: Serve documentation locally
    cmds:
      - echo "Documentation server not yet configured"

  docs:lint:
    desc: Lint all documentation (markdown + prose + file naming + cross-refs)
    cmds:
      - task: docs:lint:markdown
      - task: docs:lint:prose
      - task: docs:lint:names
      - task: docs:validate:agents
      - task: docs:validate:links
      - task: docs:validate:consistency

  docs:lint:markdown:
    desc: Lint markdown structure
    cmds:
      - npx markdownlint-cli2 "**/*.md" "#node_modules" "#.venv"

  docs:lint:prose:
    desc: Lint prose quality with Vale
    cmds:
      - vale --config=.vale.ini docs/ README.md

  docs:lint:names:
    desc: Validate file naming conventions with ls-lint
    cmds:
      - npx @ls-lint/ls-lint

  docs:validate:agents:
    desc: Validate AGENTS.md structure and links
    cmds:
      - |
        echo "Validating AGENTS.md..."
        # Check file exists
        if [ ! -f AGENTS.md ]; then
          echo "ERROR: AGENTS.md not found"
          exit 1
        fi
        # Check required sections exist
        required_sections=("Overview" "Agent Entry Template" "Agents in This Repository" "Adding or Updating Agents")
        for section in "${required_sections[@]}"; do
          if ! grep -q "## $section" AGENTS.md; then
            echo "ERROR: Missing required section: $section"
            exit 1
          fi
        done
        # Validate internal links (check files exist)
        # Skip code blocks by removing content between ``` markers
        echo "Checking internal links..."
        broken_links=0
        # Remove code blocks from file before extracting links
        content=$(awk '/^```/{f=!f;next}!f' AGENTS.md)
        while IFS= read -r link; do
          # Skip external links (http/https)
          if [[ "$link" =~ ^https?:// ]]; then
            continue
          fi
          # Skip anchor-only links
          if [[ "$link" =~ ^# ]]; then
            continue
          fi
          # Skip placeholder/template links
          if [[ "$link" =~ ^(relative/path/to|path/to) ]]; then
            continue
          fi
          # Remove anchor from link if present
          file_path="${link%%#*}"
          # Resolve relative paths from repository root
          if [ ! -f "$file_path" ] && [ ! -f "docs/$file_path" ]; then
            echo "  ✗ Broken link: $link"
            broken_links=$((broken_links + 1))
          fi
        done < <(echo "$content" | grep -oP '\[.*?\]\(\K[^)]+')
        if [ $broken_links -gt 0 ]; then
          echo "ERROR: Found $broken_links broken internal link(s)"
          exit 1
        fi
        echo "✓ AGENTS.md validation passed"

  docs:validate:links:
    desc: Validate internal markdown links across all documentation
    cmds:
      - "{{.UV}} run python scripts/validate_references.py"

  docs:validate:consistency:
    desc: Validate documentation file counts and consistency
    cmds:
      - "{{.UV}} run python scripts/validate_documentation.py"

  docs:fix:
    desc: Auto-fix markdown issues
    cmds:
      - npx markdownlint-cli2 --fix "**/*.md" "#node_modules" "#.venv"

  # Note: .windsurf/docs/ system removed in 2025-10-21
  # Rules and workflows are now self-documenting via frontmatter

  docs:clean:
    desc: Remove double-spaces and LLM artifacts
    cmds:
      - |
        find docs/ README.md -type f -name "*.md" -exec sed -i 's/  \+/ /g' {} \;
        echo "Cleaned double-spaces from documentation"

  docs:coverage:
    desc: Check documentation coverage (≥80% threshold)
    cmds:
      - "{{.UV}} run python scripts/doc_coverage.py --threshold 80"

  docs:coverage:report:
    desc: Generate documentation coverage JSON report
    cmds:
      - "{{.UV}} run python scripts/doc_coverage.py --threshold 80 --output doc-coverage.json"

  # Development tasks
  dev:setup:
    desc: Complete development environment setup
    cmds:
      - task: install
      - task: install:pre-commit
      - echo "Development environment ready!"

  dev:clean:
    desc: Clean build artifacts and caches
    cmds:
      - rm -rf build/ dist/ *.egg-info
      - rm -rf .pytest_cache/ .mypy_cache/ .ruff_cache/
      - rm -rf htmlcov/ .coverage .hypothesis/
      - find . -type d -name __pycache__ -exec rm -rf {} + 2>/dev/null || true
      - echo "Cleaned build artifacts and caches"

  dev:clean:cache:
    desc: Clean application cache
    cmds:
      - rm -rf ~/.cache/mcp-web
      - echo "Cleaned application cache"

  # Pre-commit tasks
  pre-commit:
    desc: Run pre-commit checks
    deps:
      - lint
      - test:fast

  pre-commit:all:
    desc: Run pre-commit on all files
    cmds:
      - "{{.UV}} run pre-commit run --all-files"

  # CI/CD simulation
  ci:
    desc: Simulate CI pipeline locally
    cmds:
      - echo "=== Running CI Pipeline ==="
      - task: lint
      - task: test:coverage:min
      - task: security
      - echo "=== CI Pipeline Complete ==="

  ci:fast:
    desc: Fast CI check (no coverage, parallel tests)
    cmds:
      - echo "=== Running Fast CI ==="
      - task: lint
      - task: test:fast:parallel
      - task: security:bandit
      - echo "=== Fast CI Complete ==="

  ci:parallel:
    desc: Full CI with parallel testing
    cmds:
      - echo "=== Running Parallel CI ==="
      - task: lint
      - task: test:coverage:parallel
      - task: security
      - echo "=== Parallel CI Complete ==="

  # Benchmarking tasks
  bench:
    desc: Run all benchmarks
    cmds:
      - "{{.UV}} run pytest -m benchmark --benchmark-only --benchmark-autosave"

  bench:compare:
    desc: Compare with previous benchmark
    cmds:
      - "{{.UV}} run pytest -m benchmark --benchmark-only --benchmark-compare"

  bench:profile:
    desc: Profile performance with cProfile
    cmds:
      - "{{.UV}} run python -m cProfile -o profile.stats -m pytest -m benchmark"
      - "{{.UV}} run python -c 'import pstats; p = pstats.Stats(\"profile.stats\"); p.sort_stats(\"cumulative\"); p.print_stats(30)'"

  # Docker tasks (if needed)
  docker:build:
    desc: Build Docker image
    cmds:
      - docker build -t mcp-web:latest .

  docker:test:
    desc: Run tests in Docker
    cmds:
      - docker run --rm mcp-web:latest task test

  # Release tasks
  release:check:
    desc: Check if ready for release
    cmds:
      - task: ci
      - task: test:all
      - echo "✓ Ready for release"

  release:build:
    desc: Build distribution packages
    cmds:
      - "{{.UV}} build"
      - echo "Distribution packages built in dist/"

  release:test-pypi:
    desc: Upload to TestPyPI
    cmds:
      - "{{.UV}} publish --publish-url https://test.pypi.org/legacy/"

  release:pypi:
    desc: Upload to PyPI
    cmds:
      - "{{.UV}} publish"

  # Utility tasks
  shell:
    desc: Start Python shell with package imported
    cmds:
      - "{{.UV}} run python -i -c 'from mcp_web import *; print(\"mcp-web shell ready\")'"

  deps:update:
    desc: Update dependencies
    cmds:
      - "{{.UV}} lock --upgrade"

  deps:tree:
    desc: Show dependency tree
    cmds:
      - "{{.UV}} tree"

  deps:outdated:
    desc: Check for outdated dependencies
    cmds:
      - "{{.UV}} pip list --outdated"

  # Local LLM tasks
  llm:ollama:start:
    desc: Start Ollama server (if installed)
    cmds:
      - ollama serve

  llm:ollama:pull:
    desc: Pull recommended Ollama models
    cmds:
      - ollama pull llama3.2:3b
      - ollama pull mistral:7b
      - ollama pull phi3:mini

  llm:test:local:
    desc: Test with local LLM (requires Ollama running)
    env:
      MCP_WEB_SUMMARIZER_API_BASE: http://localhost:11434/v1
      MCP_WEB_SUMMARIZER_MODEL: llama3.2:3b
      MCP_WEB_SUMMARIZER_API_KEY: ollama
    cmds:
      - "{{.UV}} run pytest -m golden -v"

  # Golden test management
  golden:update:
    desc: Update golden test expectations (use with caution)
    cmds:
      - "{{.UV}} run pytest -m golden --update-golden -v"

  golden:verify:
    desc: Verify golden tests with detailed output
    cmds:
      - "{{.UV}} run pytest -m golden -vv --tb=long"

  # Session summary mining tasks (Phase 5)
  mine:summaries:
    desc: Extract action items from session summaries (DATE_RANGE=YYYY-MM-DD:YYYY-MM-DD, OUTPUT=file.yaml)
    cmds:
      - |
        if [ -n "{{.DATE_RANGE}}" ]; then
          {{.UV}} run python scripts/extract_action_items.py --date-range {{.DATE_RANGE}}{{if .OUTPUT}} --output {{.OUTPUT}}{{end}}{{if .DB}} --db {{.DB}}{{end}}
        elif [ -n "{{.ALL}}" ]; then
          {{.UV}} run python scripts/extract_action_items.py --all{{if .OUTPUT}} --output {{.OUTPUT}}{{end}}{{if .DB}} --db {{.DB}}{{end}}
        elif [ -n "{{.FILES}}" ]; then
          {{.UV}} run python scripts/extract_action_items.py {{.FILES}}{{if .OUTPUT}} --output {{.OUTPUT}}{{end}}{{if .DB}} --db {{.DB}}{{end}}
        else
          echo "Error: Specify DATE_RANGE, ALL=true, or FILES"
          echo "Examples:"
          echo "  task mine:summaries DATE_RANGE=2025-10-15:2025-10-20"
          echo "  task mine:summaries ALL=true OUTPUT=items.yaml"
          echo "  task mine:summaries FILES='docs/archive/session-summaries/2025-10-20-*.md'"
          exit 1
        fi

  mine:summaries:dry-run:
    desc: Dry-run extraction (no API calls, validates files only)
    cmds:
      - echo "Validating session summaries..."
      - |
        if [ -n "{{.DATE_RANGE}}" ]; then
          {{.UV}} run python -c "from scripts.extract_action_items import find_summaries_by_date_range, parse_date_range; from datetime import date; start, end = parse_date_range('{{.DATE_RANGE}}'); files = find_summaries_by_date_range(start, end); print(f'Found {len(files)} summaries'); [print(f'  - {f.name}') for f in files]"
        else
          echo "Specify DATE_RANGE (e.g., DATE_RANGE=2025-10-15:2025-10-20)"
          exit 1
        fi

  # Scaffolding tasks (automation)
  # NOTE: Interactive tasks below are for HUMAN use only. AI agents should use config mode.

  scaffold:initiative:
    desc: Create new flat-file initiative from template (interactive - humans only)
    cmds:
      - "{{.UV}} run python scripts/scaffold.py --type initiative --mode interactive"

  scaffold:initiative:config:
    desc: Create initiative from config file (AI agents use this)
    cmds:
      - '[ -n "{{.CONFIG}}" ] || (echo "CONFIG is required" && exit 1)'
      - "{{.UV}} run python scripts/scaffold.py --type initiative --config {{.CONFIG}}"

  scaffold:initiative-folder:
    desc: Create new folder-based initiative with artifacts/ and phases/ (interactive - humans only)
    cmds:
      - "{{.UV}} run python scripts/scaffold.py --type initiative-folder --mode interactive"

  scaffold:adr:
    desc: Create new ADR from template (interactive - humans only)
    cmds:
      - "{{.UV}} run python scripts/scaffold.py --type adr --mode interactive"

  scaffold:adr:config:
    desc: Create ADR from config file (AI agents use this)
    cmds:
      - '[ -n "{{.CONFIG}}" ] || (echo "CONFIG is required" && exit 1)'
      - "{{.UV}} run python scripts/scaffold.py --type adr --config {{.CONFIG}}"

  scaffold:summary:
    desc: Create new session summary from template (interactive - humans only)
    cmds:
      - "{{.UV}} run python scripts/scaffold.py --type summary --mode interactive"

  scaffold:summary:config:
    desc: Create session summary from config file (AI agents use this)
    cmds:
      - '[ -n "{{.CONFIG}}" ] || (echo "CONFIG is required" && exit 1)'
      - "{{.UV}} run python scripts/scaffold.py --type summary --config {{.CONFIG}}"

  # Initiative validation tasks
  validate:initiatives:
    desc: Validate all initiative files (frontmatter, status, dependencies)
    cmds:
      - "{{.UV}} run python scripts/validate_initiatives.py"

  validate:initiatives:ci:
    desc: Validate initiatives in CI mode (exit 1 on failures)
    cmds:
      - "{{.UV}} run python scripts/validate_initiatives.py --ci"

  validate:dependencies:
    desc: Validate initiative dependencies and detect circular deps
    cmds:
      - "{{.UV}} run python scripts/dependency_registry.py --validate"

  deps:graph:
    desc: Generate initiative dependency graph (DOT format)
    cmds:
      - "{{.UV}} run python scripts/dependency_registry.py --graph"

  deps:blockers:
    desc: Show blocker propagation cascade across initiatives
    cmds:
      - "{{.UV}} run python scripts/dependency_registry.py --blockers"

  deps:export:
    desc: Export dependency registry to JSON (FILE required)
    cmds:
      - '[ -n "{{.FILE}}" ] || (echo "FILE is required" && exit 1)'
      - "{{.UV}} run python scripts/dependency_registry.py --export {{.FILE}}"

  archive:initiative:
    desc: Archive initiative from active to completed (NAME required)
    cmds:
      - '[ -n "{{.NAME}}" ] || (echo "NAME is required" && exit 1)'
      - '{{.UV}} run python scripts/file_ops.py archive-initiative {{.NAME}}{{if .COMPLETED_ON}} --completed-on {{.COMPLETED_ON}}{{end}}{{if .DRY_RUN}} --dry-run{{end}}'

  move:file:
    desc: Move file and update references (SRC and DST required)
    cmds:
      - '[ -n "{{.SRC}}" ] || (echo "SRC is required" && exit 1)'
      - '[ -n "{{.DST}}" ] || (echo "DST is required" && exit 1)'
      - '{{.UV}} run python scripts/file_ops.py move-file {{.SRC}} {{.DST}}{{if .NO_UPDATE_REFS}} --no-update-refs{{end}}{{if .DRY_RUN}} --dry-run{{end}}'

  update:index:
    desc: Update documentation index (defaults to docs/initiatives)
    cmds:
      - '{{.UV}} run python scripts/file_ops.py update-index {{default "docs/initiatives" .DIR}}{{if .DRY_RUN}} --dry-run{{end}}'

  # Information tasks
  info:
    desc: Show project information
    cmds:
      - echo "Project{{\":\"}} mcp-web"
      - "{{.UV}} run python --version"
      - "{{.UV}} run pytest --version"
      - "{{.UV}} run ruff --version"
      - echo "Coverage minimum{{\":\"}} {{.COVERAGE_MIN}}%"
      - echo "Parallel workers{{\":\"}} {{.PARALLEL_WORKERS}}"

  info:env:
    desc: Show environment configuration
    cmds:
      - "{{.UV}} run python -c 'from mcp_web.config import MCPWebConfig; import json; print(json.dumps(MCPWebConfig().model_dump(), indent=2))'"
